![image](https://github.com/user-attachments/assets/a71e95a5-34f6-4a5b-b698-54560c7a09d6)

# üèÜ KO-SAT Slayer Champions League

Welcome to the **KO-SAT Slayer Champions League**! This leaderboard is designed to identify the ultimate Slayer of the Korean SAT (Suneung)! üöÄ

Check how your Korean language model, fine-tuned for the Korean SAT, performs on this 10-year benchmark!

If you can provide access to GPU resources, it would greatly assist in the evaluation process. Thank you!

## üéØ What is the KO-SAT Slayer Champions League Leaderboard?

The KO-SAT Slayer Champions League is a leaderboard benchmarked using the Korean SAT (Suneung) language exams from the past 10 years, developed by KICE (Korea Institute for Curriculum and Evaluation), a respected authority in the field. The Korean SAT features a range of question types that assess reading comprehension, critical thinking, and sentence interpretation skills.

## üèÜ Hall of Fame

- **Ranking Criteria**: The average of standardized scores across 10 years (standardized scores reflect the difficulty of each year's exam).
- **Score Format**: Raw score (Grade)

[Click here for a detailed explanation of scoring]()

| Leaderboard Rank | Model Name                         | Submitter Name | Avg. Korean SAT std Score (2015-2024) | Avg. Korean SAT Raw Score (2015-2024) | Avg. Grade | 2015 SAT | 2016 SAT | 2017 SAT | 2018 SAT | 2019 SAT | 2020 SAT | 2021 SAT | 2022 SAT | 2023 SAT | 2024 SAT | URL                                                                                                                                    |
|-----------------:|:-----------------------------------|:---------------|--------------------------------------:|--------------------------------------:|-----------:|:---------|:---------|:---------|:---------|:---------|:---------|:---------|:---------|:---------|:---------|:---------------------------------------------------------------------------------------------------------------------------------------|
|           ü•á 1st | gpt-4o                             | OpenAI         |                                 114.9 |                                  75.9 |        3.6 | 77 (4)   | 84 (3)   | 86 (2)   | 77 (4)   | 74 (3)   | 76 (4)   | 69 (4)   | 70 (4)   | 81 (4)   | 65 (4)   | [Link](https://openai.com/)                                                                                                            |
|           ü•à 2nd | Meta-Llama-3.1-405B-Instruct-Turbo | meta-llama     |                                 113.8 |                                  74.9 |        3.8 | 68 (5)   | 87 (3)   | 80 (3)   | 78 (4)   | 68 (4)   | 65 (5)   | 70 (4)   | 69 (4)   | 87 (3)   | 77 (3)   | [Link](https://huggingface.co/meta-llama/Llama-3.1-405B-Instruct)                                                                      |
|           ü•â 3rd | Meta-Llama-3.1-70B-Instruct-Turbo  | meta-llama     |                                 103.7 |                                  65.1 |        4.8 | 70 (5)   | 71 (5)   | 66 (5)   | 58 (6)   | 51 (5)   | 79 (3)   | 61 (5)   | 73 (3)   | 72 (5)   | 50 (6)   | [Link](https://huggingface.co/meta-llama/Llama-3.1-70B-Instruct)                                                                       |
|              4th | Qwen2-72B-Instruct                 | Qwen           |                                    98 |                                  59.3 |        5.2 | 63 (5)   | 58 (6)   | 69 (5)   | 76 (4)   | 56 (5)   | 57 (5)   | 45 (6)   | 59 (5)   | 57 (6)   | 53 (5)   | [Link](https://huggingface.co/Qwen)                                                                                                    |
|              5th | gpt-4o-mini                        | OpenAI         |                                  93.9 |                                  55.2 |        5.6 | 57 (6)   | 64 (5)   | 58 (6)   | 62 (5)   | 46 (6)   | 50 (6)   | 55 (5)   | 50 (6)   | 53 (6)   | 57 (5)   | [Link](https://openai.com/)                                                                                                            |
|              6th | gemma-2-27b-it                     | Google         |                                    91 |                                  52.5 |        5.9 | 56 (6)   | 54 (6)   | 71 (4)   | 50 (6)   | 37 (7)   | 50 (6)   | 51 (6)   | 51 (6)   | 54 (6)   | 51 (6)   | [Link](https://huggingface.co/google/gemma-2-27b-it)                                                                                   |
|              7th | solar-mini-ja                      | Upstage        |                                  85.9 |                                  47.4 |        6.2 | 52 (6)   | 40 (7)   | 48 (7)   | 49 (6)   | 51 (5)   | 46 (6)   | 41 (7)   | 43 (6)   | 58 (6)   | 46 (6)   | [Link](https://ko.upstage.ai/feed/company/event-recap-exploring-japan-ai-scene-with-upstage-solar-mini-ja)                             |
|              8th | solar-mini                         | Upstage        |                                  85.5 |                                  47.2 |        6.4 | 56 (6)   | 42 (7)   | 55 (6)   | 43 (7)   | 50 (6)   | 46 (6)   | 42 (7)   | 48 (6)   | 57 (6)   | 33 (7)   | [Link](https://www.upstage.ai/feed/product/solarmini-performance-report)                                                               |
|              9th | Mixtral-8x22B-Instruct-v0.1        | MistralAI      |                                  83.4 |                                  45.1 |        6.6 | 44 (7)   | 50 (6)   | 57 (6)   | 65 (5)   | 35 (7)   | 38 (7)   | 31 (8)   | 47 (6)   | 44 (7)   | 40 (7)   | [Link](https://huggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1)                                                                   |
|             10th | WizardLM-2-8x22B                   | Microsoft      |                                  83.3 |                                  45.3 |        6.6 | 53 (6)   | 51 (6)   | 47 (7)   | 51 (6)   | 29 (8)   | 52 (6)   | 30 (8)   | 47 (6)   | 56 (6)   | 37 (7)   | [Link](https://www.microsoft.com/en-us/research/publication/wizardlm-empowering-large-language-models-to-follow-complex-instructions/) |
|             11th | Meta-Llama-3.1-8B-Instruct-Turbo   | meta-llama     |                                  74.7 |                                  36.5 |        7.1 | 45 (7)   | 37 (7)   | 38 (7)   | 38 (7)   | 24 (8)   | 36 (7)   | 34 (7)   | 36 (7)   | 31 (8)   | 46 (6)   | [Link](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct)                                                                        |
|             12th | gpt-3_5-turbo                      | OpenAI         |                                  68.7 |                                  30.6 |        7.7 | 27 (8)   | 45 (7)   | 25 (8)   | 38 (7)   | 24 (8)   | 36 (7)   | 17 (9)   | 26 (8)   | 39 (7)   | 29 (8)   | [Link](https://openai.com/)                                                                                                            |
|             13th | Mixtral-8x7B-Instruct-v0.1         | MistralAI      |                                  63.4 |                                  25.2 |        8.3 | 19 (9)   | 30 (8)   | 16 (9)   | 37 (7)   | 19 (9)   | 27 (8)   | 20 (9)   | 40 (7)   | 25 (8)   | 19 (9)   | [Link](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1)                                                                    |
|             14th | gemma-2-9b-it                      | Google         |                                  61.2 |                                  23.1 |        8.4 | 29 (8)   | 25 (8)   | 25 (8)   | 24 (8)   | 29 (8)   | 17 (9)   | 22 (9)   | 16 (9)   | 20 (9)   | 24 (8)   | [Link](https://huggingface.co/google/gemma-2-9b-it)                                                                                    |
|             15th | Llama-3.2-3B-Instruct-Turbo        | meta-llama     |                                  60.6 |                                  22.4 |        8.7 | 23 (9)   | 22 (9)   | 29 (8)   | 21 (9)   | 17 (9)   | 16 (9)   | 23 (9)   | 27 (8)   | 18 (9)   | 28 (8)   | [Link](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct)                                                                        |
|             16th | Mistral-7B-Instruct-v0.3           | MistralAI      |                                  57.2 |                                  19.1 |        8.9 | 21 (9)   | 23 (9)   | 27 (8)   | 19 (9)   | 21 (9)   | 18 (9)   | 12 (9)   | 22 (9)   | 11 (9)   | 17 (9)   | [Link](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3)                                                                      |

### üìó Notes: Performance Comparison for 2024 Korean SAT (Single-Year Results)

- **o1-preview**: 88 points (Grade 1, Top 4%)
- **o1-mini**: 60 points (Grade 5)

## üèÖ Submission Guidelines

- If you prefer to keep your model‚Äôs performance private and not appear on the public leaderboard, feel free to leave a note in the "Comments" section.
- ‚≠êÔ∏è Your model must have a **minimum context length of 8K tokens** to solve the Korean SAT questions!

1. **Model Submission**:
    - **[Submit via the Form](https://moaform.com/q/X6xfGE)**: Fill in the survey form to submit your model!
        - Link: https://moaform.com/q/X6xfGE
    - **Email Submission**: Send the Hugging Face URL of your fine-tuned model along with your nickname.
        - Submission email: developerminsing@gmail.com
    - **Submit via GitHub Issue**: Post your model‚Äôs Hugging Face URL and nickname in a GitHub issue.
    ```markdown
    <Example for email or GitHub issue submission>
    Submitter Name: Elon
    Hugging Face Submission URL: https://huggingface.co/Elon_model
    Comments: Let's go Mars!
    ```

2. **Check the Leaderboard**: View your rank on GitHub or Hugging Face.
3. **Climb the Ranks**: Improve your score and claim the **Slayer Champion** title!

**Notice:** Evaluation may take 1-3 weeks depending on available GPU resources and submission volume.

## ü™ë Benchmark Dataset

- The competition uses the Korean SAT language exams from 2015 to 2024. The models will be evaluated on questions and passages from these exams, which assess reading comprehension, critical thinking, and sentence interpretation abilities.
- Key evaluation areas for the benchmark dataset include language comprehension, identifying key points, logical thinking, critical analysis, creative reasoning, and multimedia interpretation.
  <Source: 2024 KICE Korean SAT evaluation list>

## ‚ôæÔ∏è Metrics

### Evaluation Method
- Each model's answer is compared with the correct answer, and the final score is based on how many answers match.
- Scores are calculated per year‚Äôs exam, and the final ranking is determined by the average standardized score.

### Leaderboard Scoring Explanation
- **Raw Score**: The score out of 100 that a student received on the exam.
- **Standardized Score**: A score that reflects how far a student‚Äôs raw score deviates from the average.
- **Grade**: Based on the standardized score, students are divided into 9 grades. For Korean language, math, and social studies/sciences, the top 4% of students receive Grade 1, the next 7% (up to 11% cumulative) receive Grade 2, and the next 12% (up to 23% cumulative) receive Grade 3.
[Reference: EBSI](https://www.ebsi.co.kr/ebs/ent/enta/retrieveEntNewsView.ebs?bbsCd=B011&datNo=142017)

## üìó Helpful References

- [Nomadamas Experiment Log](https://github.com/NomaDamas/KICE_slayer_AI_Korean?tab=readme-ov-file#5-%ED%98%95%EC%8B%9D-%EC%A7%80%EC%A0%95-%ED%94%84%EB%A1%AC%ED%94%84%ED%8A%B8)

## üì∞ Notice

- Due to copyright issues, the benchmark dataset for the Korean SAT will not be made public. The evaluation will be
  based on the 2015 to 2024 exams.
- To ensure fairness, the exact prompts will not be revealed!
- Rankings will be based on the average scores across the 10 years of the Korean SAT.
- Updates for all subject areas (English, Mathematics, Social Studies, and Science) will be added to the leaderboard in
  the future.

## üì¨ Contact Us

- For questions, errors, or support, feel free to reach out:

- Email: developerminsing@gmail.com

Are you ready to become the next **KO-SAT Slayer Champion**? üí™

