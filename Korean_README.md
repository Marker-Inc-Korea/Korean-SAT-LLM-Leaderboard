
# 🏆 수능 국어 LLM 리더보드

------------
"인간시대의 끝이 도래했다~" 수능 국어 LLM이 사람보다 국어 문제를 더 잘 푼다~? 

수능 국어 LLM 벤치마크 리더보드를 바탕으로, 사람의 수학 능력과 LLM의 성능을 비교할 수 있는 특별한 기회에 여러분도 참여해보세요!

여러분이 개발한 한국어 LLM 파인튜닝 모델이 한국 수능 10개년 벤치마크에서 몇 점을 받을지 지금 바로 확인해보세요!

| Leaderboard Rank |             Model Name             | Submitter Name | Avg. std Score | Avg. Grade | 2024 SAT | 2023 SAT | 2022 SAT | 2021 SAT | 2020 SAT | 2019 SAT | 2018 SAT | 2017 SAT | 2016 SAT | 2015 SAT | URL                                                                                                                                    |
|:----------------:|:----------------------------------:|:--------------:|:--------------:|:----------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:---------------------------------------------------------------------------------------------------------------------------------------|
|    🥇 **1st**    |         gpt-4o-2024-08-06          |     OpenAI     |     114.9      |    3.6     |  65 (4)  |  81 (4)  |  70 (4)  |  69 (4)  |  76 (4)  |  74 (3)  |  77 (4)  |  86 (2)  |  84 (3)  |  77 (4)  | [Link](https://openai.com/)                                                                                                            |
|    🥈 **2nd**    | Meta-Llama-3.1-405B-Instruct-Turbo |   meta-llama   |     113.8      |    3.8     |  77 (3)  |  87 (3)  |  69 (4)  |  70 (4)  |  65 (5)  |  68 (4)  |  78 (4)  |  80 (3)  |  87 (3)  |  68 (5)  | [Link](https://huggingface.co/meta-llama/Llama-3.1-405B-Instruct)                                                                      |
|    🥉 **3rd**    | Meta-Llama-3.1-70B-Instruct-Turbo  |   meta-llama   |     103.7      |    4.8     |  50 (6)  |  72 (5)  |  73 (3)  |  61 (5)  |  79 (3)  |  51 (5)  |  58 (6)  |  66 (5)  |  71 (5)  |  70 (5)  | [Link](https://huggingface.co/meta-llama/Llama-3.1-70B-Instruct)                                                                       |
|       4th        |         Qwen2-72B-Instruct         |      Qwen      |       98       |    5.2     |  53 (5)  |  57 (6)  |  59 (5)  |  45 (6)  |  57 (5)  |  56 (5)  |  76 (4)  |  69 (5)  |  58 (6)  |  63 (5)  | [Link](https://huggingface.co/Qwen)                                                                                                    |
|       5th        |       gpt-4o-mini-2024-07-18       |     OpenAI     |      93.9      |    5.6     |  57 (5)  |  53 (6)  |  50 (6)  |  55 (5)  |  50 (6)  |  46 (6)  |  62 (5)  |  58 (6)  |  64 (5)  |  57 (6)  | [Link](https://openai.com/)                                                                                                            |
|       6th        |           gemma-2-27b-it           |     Google     |       91       |    5.9     |  51 (6)  |  54 (6)  |  51 (6)  |  51 (6)  |  50 (6)  |  37 (7)  |  50 (6)  |  71 (4)  |  54 (6)  |  56 (6)  | [Link](https://huggingface.co/google/gemma-2-27b-it)                                                                                   |
|       7th        |           solar-mini-ja            |    Upstage     |      85.9      |    6.2     |  46 (6)  |  58 (6)  |  43 (6)  |  41 (7)  |  46 (6)  |  51 (5)  |  49 (6)  |  48 (7)  |  40 (7)  |  52 (6)  | [Link](https://ko.upstage.ai/feed/company/event-recap-exploring-japan-ai-scene-with-upstage-solar-mini-ja)                             |
|       8th        |             solar-mini             |    Upstage     |      85.5      |    6.4     |  33 (7)  |  57 (6)  |  48 (6)  |  42 (7)  |  46 (6)  |  50 (6)  |  43 (7)  |  55 (6)  |  42 (7)  |  56 (6)  | [Link](https://www.upstage.ai/feed/product/solarmini-performance-report)                                                               |
|       9th        |    Mixtral-8x22B-Instruct-v0.1     |   MistralAI    |      83.4      |    6.6     |  40 (7)  |  44 (7)  |  47 (6)  |  31 (8)  |  38 (7)  |  35 (7)  |  65 (5)  |  57 (6)  |  50 (6)  |  44 (7)  | [Link](https://huggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1)                                                                   |
|       10th       |          WizardLM-2-8x22B          |   Microsoft    |      83.3      |    6.6     |  37 (7)  |  56 (6)  |  47 (6)  |  30 (8)  |  52 (6)  |  29 (8)  |  51 (6)  |  47 (7)  |  51 (6)  |  53 (6)  | [Link](https://www.microsoft.com/en-us/research/publication/wizardlm-empowering-large-language-models-to-follow-complex-instructions/) |
|       11th       |  Meta-Llama-3.1-8B-Instruct-Turbo  |   meta-llama   |      74.7      |    7.1     |  46 (6)  |  31 (8)  |  36 (7)  |  34 (7)  |  36 (7)  |  24 (8)  |  38 (7)  |  38 (7)  |  37 (7)  |  45 (7)  | [Link](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct)                                                                        |
|       12th       |           gpt-3_5-turbo            |     OpenAI     |      68.7      |    7.7     |  29 (8)  |  39 (7)  |  26 (8)  |  17 (9)  |  36 (7)  |  24 (8)  |  38 (7)  |  25 (8)  |  45 (7)  |  27 (8)  | [Link](https://openai.com/)                                                                                                            |
|       13th       |     Mixtral-8x7B-Instruct-v0.1     |   MistralAI    |      63.4      |    8.3     |  19 (9)  |  25 (8)  |  40 (7)  |  20 (9)  |  27 (8)  |  19 (9)  |  37 (7)  |  16 (9)  |  30 (8)  |  19 (9)  | [Link](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1)                                                                    |
|       14th       |           gemma-2-9b-it            |     Google     |      61.2      |    8.4     |  24 (8)  |  20 (9)  |  16 (9)  |  22 (9)  |  17 (9)  |  29 (8)  |  24 (8)  |  25 (8)  |  25 (8)  |  29 (8)  | [Link](https://huggingface.co/google/gemma-2-9b-it)                                                                                    |
|       15th       |    Llama-3.2-3B-Instruct-Turbo     |   meta-llama   |      60.6      |    8.7     |  28 (8)  |  18 (9)  |  27 (8)  |  23 (9)  |  16 (9)  |  17 (9)  |  21 (9)  |  29 (8)  |  22 (9)  |  23 (9)  | [Link](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct)                                                                        |
|       16th       |      Mistral-7B-Instruct-v0.3      |   MistralAI    |      57.2      |    8.9     |  17 (9)  |  11 (9)  |  22 (9)  |  12 (9)  |  18 (9)  |  21 (9)  |  19 (9)  |  27 (8)  |  23 (9)  |  21 (9)  | [Link](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3)                                                                      |

- **Rank 기준**: 10개년 수능 표준점수들의 평균  <표준점수를 통해서 각 년도별 시험의 난이도를 점수에서 반영합니다.>
- **Avg. std Score:** 표준점수 평균
- **Avg. Grade:** 등급 평균
- **년도별 수능 점수 표기**: 원점수(등급)

[점수에 대한 설명 바로가기](https://github.com/minsing-jin/KO-SAT_Slayer_Champions_League/blob/main/Korean_README.md#%EF%B8%8F-metric)

i.e)
GPU 자원 Donation해주신다면 평가에 큰 도움이 될 것 같습니다. 감사합니다!

### 📗 Notes. 24 수능 (1개년) 모델 성능 비교 결과

- **o1-preview**: 88점 (1등급, 상위 4%)
- **o1-mini** : 60점 (5등급)

---

## 🎯 수능 국어 LLM Leaderboard란?

KO-SAT Slayer Champions League는 한국교육과정평가원(KICE)이 개발한 대학수학능력시험(수능) 국어 과목의 10개년 시험문제를 기반으로 한 벤치마크 리더보드입니다.

매년 엄선된 수능 국어 시험 문제를 통해 여러분의 대형 언어 모델(LLM)의 성능을 평가할 수 있는 기회를 제공합니다. 평가 방식은 실제 수능과 동일하게 표준점수와 등급 체계를 사용하여, 사람의 시험 성적과 LLM의 성능을 직접 비교할 수 있도록 구성되어 있습니다.

여러분의 모델이 수능 국어 시험에서 얼마나 우수한 성과를 낼 수 있는지 도전해 보세요!

## 🏅 Submit 방식

- 리더보드 공개를 원하지 않고, private하게 모델의 성능을 알고 싶다면 하고 싶은 말 파트에 남겨주세요!
- ⭐️ 수능 문제를 풀기 위한 모델의 context length는 최소 8K 이상이어야합니다!

1. **모델 submission**:
    - **[설문 Form으로 제출](https://moaform.com/q/QP0AV0)**: 설문 응답에 맞춰 작성해주세요!
        - 링크: https://moaform.com/q/QP0AV0
    - **이메일로 제출**: huggingFace에 게시된 자신의 finetuning 모델의 Url과 닉네임을 전송해주세요!
        - 제출 메일: developerminsing@gmail.com
    - **issue로 제출**: Github의 이슈에서 자신의 finetuning 모델의 Url과 닉네임을 게시해주세요!
    ```markdown
   <이메일 제출, 이슈 제출시 Form example>
    제출자 이름: 감스트
    HuggingFace 제출 URL: https://huggingface.co/감스트모델짜스
    하고 싶은말: 열심히 하시잖아
    ```

2. **리더보드 확인**: github와 huggingFace에서 자신의 순위를 확인할 수 있습니다.
3. **순위 상승 도전**: 자신의 순위를 올려 **Slayer Champion** 타이틀을 획득하세요.

**Notice:** 모델 제출후 가용한 GPU 리소스와, 제출량에 따라 1~3주일의 시간이 소요될 수 있습니다.

## 🪑 Benchmark 데이터셋

- 본 대회에서는 2015년부터 2024년까지의 10개년 수능 국어 문제를 사용합니다.
- 2022년도부터 시행된 선택과목에 대해서는 화법과 작문 과목 선택과목으로 하여 benchmark를 진행합니다.
- Benchmark 데이터셋의 주요 평가 목록은 언어 이해력, 핵심 내용 파악 능력, 논리적 사고력, 비판적 사고력, 창의적 사고력, 멀티미디어 해석력을 평가합니다.
  <출처: 2024 KICE 수능 국어 평가 목록>

## ♾️ Metric

### 평가 방식

- 대회에서는 각 모델이 제시된 문제에 대해 제출한 답안이 실제 정답과 일치하는지 여부를 측정하는 방식입니다.
- 평가 점수는 각 년도의 문제별로 채점되며, 최종적으로는 표준점수의 평균을 통해 순위가 매겨집니다.

### 리더보드 점수 설명

- **원점수란?**: 시험에서 100점만점으로 받은 점수
- **표준점수**: 응시생이 받은 원점수가 평균에서 얼마나 떨어져 있는지 일종의 '평균과의 거리'를 측정하는 점수
- **등급**: 표준점수에 근거해 수험생을 나눈 것으로, 총 9등급이 있다. 국어와 수학, 탐구영역에서는 영역과목별 전체 수험생의 상위 4%가 1등급, 그다음 7%(누적 11%)까지가 2등급, 그다음 12%(누적
  23%)까지가 3등급이 된다.
  [EBSI 참고](https://www.ebsi.co.kr/ebs/ent/enta/retrieveEntNewsView.ebs?bbsCd=B011&datNo=142017)

## 📗 참고해볼만한 Reference

- [Nomadamas 실험기록](https://github.com/NomaDamas/KICE_slayer_AI_Korean?tab=readme-ov-file#5-%ED%98%95%EC%8B%9D-%EC%A7%80%EC%A0%95-%ED%94%84%EB%A1%AC%ED%94%84%ED%8A%B8)

## 📰 Notice

- 저작권 문제가 있을수 있어 수능 벤치마크 데이터셋은 공개하지 않을 예정입니다. 평가 데이터는 15수능 ~ 24수능이며 22년도~24년도 선태과목은 화법과 작문에 대해서만 평가합니다.
- 평가의 공정성을 위해서 프롬프트는 공개하지 않습니다!
- 추후 수능 당일 제출해주신 모델들을 전부 반영할 국영수사과 모두 리더보드에서 업데이트 예정입니다.

## 📬 문의하기

- 궁금한 점이나 오류, 지원이 필요하다면 언제든지 연락해 주세요:

- 이메일: developerminsing@gmail.com

**다음 KO-SAT Slayer Champion**이 될 준비가 되셨나요? 💪

